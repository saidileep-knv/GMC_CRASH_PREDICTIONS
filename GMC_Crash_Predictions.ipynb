{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "GMC Crash Predictions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saidileep-knv/GMC_CRASH_PREDICTIONS/blob/master/GMC_Crash_Predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivCOGeXYRKJG",
        "colab_type": "text"
      },
      "source": [
        "The data consists of 2,375 complaints about specific GMC vehicles submitted to the National Highway Safety and Traffic Administration (NHTSA).\n",
        "\n",
        "The data dictionary is as follows:\n",
        "\n",
        "nthsa_id: A unique number for each complaint\n",
        "Year: The car year - 2003 thru 2011\n",
        "make: The make of the car - Chevrolet, Pontiac, Saturn\n",
        "model: The car model - Cobalt, G5, HHR, ION, SKY, SOLSTICE\n",
        "description: The actual complaint in text format\n",
        "crashed: A binary attribute - 'N' for no and 'Y' for yes\n",
        "abs: Anti-Brake System - 'N' for no and 'Y' for yes\n",
        "mileage: The miles on the car at the time of the accident - 0 to 200,000\n",
        "\n",
        "\n",
        "Objective: To build a model that predicts whether the car was involved in a crash using the complaint and automobile characteristics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMclzSN2S_ft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "d44ced1e-e502-4a42-e113-304ed46ba730"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD1Vgt2ETCYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"./gdrive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xko9SnCRRKJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K9NQdcGTi4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e5c266ab-97d2-4b95-c726-023823db3872"
      },
      "source": [
        "!pip install newspaper3k\n",
        "!pip install newsapi-python\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/90/18ac0e5340b6228c25cc8e79835c3811e7553b2b9ae87296dfeb62b7866d/tldextract-2.2.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 17.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.5.3)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.21.0)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 45.5MB/s \n",
            "\u001b[?25hCollecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (41.2.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.8)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading https://files.pythonhosted.org/packages/23/9c/6e63c23c39e53d3df41c77a3d05a49a42c4e1383a6d2a5e3233161b89dbf/requests_file-1.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow>=3.3.0->newspaper3k) (0.46)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.3->newspaper3k) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2019.6.16)\n",
            "Building wheels for collected packages: jieba3k, feedparser, tinysegmenter, feedfinder2\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398407 sha256=01cb0f7b44aa1ede6e45456e06bc80c36d8dc8d7f68ef1b59ed3fc39ec356723\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedparser: filename=feedparser-5.2.1-cp36-none-any.whl size=44940 sha256=0bcd56549f52ae946a42d4518e4f8d4614f379d903d47d84255c1454c7ff6541\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13538 sha256=2df8ac9e2ab59c608e3043aadd51065b42678c0fbd164c0973c7db3890449377\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3356 sha256=936632861f6a49099b14d7ae3475d2e66e814336f66aa58c20d1d6baff258c16\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "Successfully built jieba3k feedparser tinysegmenter feedfinder2\n",
            "Installing collected packages: cssselect, requests-file, tldextract, jieba3k, feedparser, tinysegmenter, feedfinder2, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.4.3 tinysegmenter-0.3 tldextract-2.2.1\n",
            "Collecting newsapi-python\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/49/cbb39846c53076a1cde2c73c5dbc3d39956ea7586c8dfc35d516d706a497/newsapi-python-0.2.5.tar.gz\n",
            "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from newsapi-python) (2.21.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->newsapi-python) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->newsapi-python) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->newsapi-python) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->newsapi-python) (2019.6.16)\n",
            "Building wheels for collected packages: newsapi-python\n",
            "  Building wheel for newsapi-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for newsapi-python: filename=newsapi_python-0.2.5-py2.py3-none-any.whl size=6593 sha256=754698aa6140055d8b86d11b79ff0f891e69290e5f3d3abece6765d4db4b0ef5\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/dc/5e/857ef20410a023cfe728ac6f360958df44b199b459cbc6ccbc\n",
            "Successfully built newsapi-python\n",
            "Installing collected packages: newsapi-python\n",
            "Successfully installed newsapi-python-0.2.5\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAHmSD4qRKJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import AdvancedAnalytics\n",
        "from AdvancedAnalytics import ReplaceImputeEncode\n",
        "from AdvancedAnalytics import logreg\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from AdvancedAnalytics import DecisionTree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import export_graphviz\n",
        "from pydotplus.graphviz import graph_from_dot_data\n",
        "import graphviz\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHCBGwUyRKJa",
        "colab_type": "text"
      },
      "source": [
        "Helper Function: my_analyzer(s), called by the sklearn Count and TDIDF Vectorizers\n",
        "\n",
        "The following helper function will be used to customize the parse, pos, stop, stem process necessary for text analysis. These are done using the NLTK package, customized to remove certain words and symbols, and handle sysnonyms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMA54Z4ARKJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_analyzer(s):\n",
        "    ##Synonym List\n",
        "    syns = {'veh':'vehicle', 'car':'vehicle', 'chev':'chevrolet',\n",
        "           'chevy':'chevrolet', 'air bag':'airbag', \"n't\":'not',\n",
        "           'seat belt':'seatbelt','to30':'to 30', 'wont':'would not',\n",
        "           'cant':'can not', 'cannot':'can not', 'couldnt':'could not',\n",
        "           'shouldnt':'should not', 'wouldnt':'would not',\n",
        "           'starightforward':'straight forward'}\n",
        "    \n",
        "    ##Preprocess String s\n",
        "    s = s.lower()\n",
        "    ##Replace special characters with spaces\n",
        "    s = s.replace('_',' ')\n",
        "    s = s.replace('-',' ')\n",
        "    s = s.replace(',','. ')\n",
        "    ##Replace not contraction with 'not'\n",
        "    s = s.replace(\"'nt\",' not')\n",
        "    s = s.replace(\"n't\",' not')\n",
        "    ##Tokenize\n",
        "    tokens = word_tokenize(s)\n",
        "    tokens = [word for word in tokens if ('*' not in word) and\n",
        "             (\"''\"!=word) and (\"``\"!=word) and (word!='description')\n",
        "             and (word!='dtype') and (word!='object') and (word!=\"'s\")]\n",
        "    \n",
        "    ##Map synonyms\n",
        "    for i in range(len(tokens)):\n",
        "        if tokens[i] in syns:\n",
        "            tokens[i] = syns[tokens[i]]\n",
        "    \n",
        "    ##Remove Stop Words\n",
        "    punctuation = list(string.punctuation)+['..','...']\n",
        "    pronouns = ['i', 'he', 'she', 'it', 'him', 'they', 'we', 'us', 'them']\n",
        "    others = [\"'d\", \"co\", \"ed\", \"put\", \"say\", \"get\", \"can\", \"become\",\n",
        "              \"los\", \"sta\", \"la\", \"use\", \"iii\", \"else\"]\n",
        "    stop = stopwords.words('english')+punctuation+pronouns+others\n",
        "    filtered_terms = [word for word in tokens if (word not in stop) and\n",
        "                     (len(word)>1) and (not word.replace('.','',1).isnumeric())\n",
        "                     and (not word.replace(\"'\",'',2).isnumeric())]\n",
        "    \n",
        "    # Lemmatization & Stemming - Stemming with WordNet POS    \n",
        "    # Since lemmatization requires POS need to set POS    \n",
        "    tagged_words = pos_tag(filtered_terms, lang='eng')    \n",
        "    # Stemming for terms without WordNet POS    \n",
        "    stemmer = SnowballStemmer(\"english\")    \n",
        "    wn_tags = {'N':wn.NOUN, 'J':wn.ADJ, 'V':wn.VERB, 'R':wn.ADV}    \n",
        "    wnl = WordNetLemmatizer()    \n",
        "    stemmed_tokens = []    \n",
        "    for tagged_token in tagged_words:        \n",
        "        term = tagged_token[0]        \n",
        "        pos  = tagged_token[1]        \n",
        "        pos  = pos[0]        \n",
        "        try:            \n",
        "            pos   = wn_tags[pos]            \n",
        "            stemmed_tokens.append(wnl.lemmatize(term, pos=pos))       \n",
        "        except:            \n",
        "            stemmed_tokens.append(stemmer.stem(term))    \n",
        "    return stemmed_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7NjBYxXRKJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_topics(lda, terms, n_terms=15):\n",
        "    for topic_idx, topic in enumerate(lda):\n",
        "        if topic_idx>8:\n",
        "            break\n",
        "        message = \"Topic #%d: \" %(topic_idx+1)\n",
        "        print(message)\n",
        "        abs_topic = abs(topic)\n",
        "        topic_terms_sorted = [[terms[i], topic[i]] \n",
        "                              for i in abs_topic.argsort()[:-n_terms -1:-1]]\n",
        "        k = 5\n",
        "        n = int(n_terms/k)\n",
        "        m = n_terms-k*n\n",
        "        for j in range(n):\n",
        "            l = k*j\n",
        "            message = \"\"\n",
        "            for i in range(k):\n",
        "                if topic_terms_sorted[i+l][1]>0:\n",
        "                    word = \"+\"+topic_terms_sorted[i+l][0]\n",
        "                else:\n",
        "                    word = \"-\"+topic_terms_sorted[i+l][0]\n",
        "                message += '{:<15s}'.format(word)\n",
        "            print(message)\n",
        "        if m>0:\n",
        "            l = k*n\n",
        "            message = \"\"\n",
        "            for i in range(m):\n",
        "                if topic_terms_sorted[i+l][1]>0:\n",
        "                    word = \"+\"+topic_terms_sorted[i+l][0]\n",
        "                else:\n",
        "                    word = \"-\"+topic_terms_sorted[i+l][0]\n",
        "                message += '{:<15s}'.format(word)\n",
        "            print(message)\n",
        "        print(\"\")\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbaiFx33RKJx",
        "colab_type": "text"
      },
      "source": [
        "Read the Data File:\n",
        "\n",
        "The maximum column width in pandas is increased to ensure the text is read without truncation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Li8PSCRKJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Increase column width to let pandas read large text columns\n",
        "pd.set_option('max_colwidth', 32000)\n",
        "\n",
        "##Read NHTSA comments\n",
        "df = pd.read_excel(\"GMC_Complaints.xlsx\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-LR0lS6RKKK",
        "colab_type": "code",
        "outputId": "05b5ede2-3105-4429-ee95-758994ecec34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "df.head(4)\n",
        "df.dtypes"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nthsa_id</th>\n",
              "      <th>Year</th>\n",
              "      <th>make</th>\n",
              "      <th>model</th>\n",
              "      <th>description</th>\n",
              "      <th>crashed</th>\n",
              "      <th>abs</th>\n",
              "      <th>mileage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10022578</td>\n",
              "      <td>2003</td>\n",
              "      <td>SATURN</td>\n",
              "      <td>ION</td>\n",
              "      <td>WHILE TRAVELING ON THE HIGHWAY AND WITHOUT PRIOR WARNING SEAT BELT RETRACTOR FELL APART.  *AK  THE BOLT THAT CONNECTS THE WEBBING TO THE FLOOR WAS NOT FULLY SCREWED IN AT THE PLANT.  THE BOLT BACKED OUT AND THE LOWER PORTION OF THE SEATBELT WEBBING BECAME UNATTACHED.  THIS IS NOT A BUCKLE ISSUE OR A RETRACTOR ISSUE.  MANUFACTURING DEFECT FROM THE PLANT BECAUSE THE BOLT WAS NOT FULLY TORQUED.  DEALER FIXED BY TIGHTENING THE BOLT.  CW</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10040419</td>\n",
              "      <td>2003</td>\n",
              "      <td>SATURN</td>\n",
              "      <td>ION</td>\n",
              "      <td>WHILE DRIVING TRANSMISSION DOES NOT ENGAGE PROPERLY,   CAUSING VEHICLE TO STALL. *AK</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10042851</td>\n",
              "      <td>2003</td>\n",
              "      <td>SATURN</td>\n",
              "      <td>ION</td>\n",
              "      <td>IN A PANIC SITUATION, THE OWNER WAS UNABLE TO LOCATE THE HORN BUTTON DUE TO  THE SIZE AND LOCATION. THIS CAUSED A  DISTRACTION, DUE TO THE  CONSUMER HAVING TO TAKE HER  EYES OFF THE ROAD AND LOOK ON THE STEERING WHEEL TO LOCATE THE HORN BUTTON.*AK    THE CONSUMER NOTED THE DRIVER'S HEAD REST WAS TILTED TOO FAR FORWARD, THE PROBLEM WAS EVENTUALLY CORRECTED.  *JB    *NM</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10049638</td>\n",
              "      <td>2003</td>\n",
              "      <td>SATURN</td>\n",
              "      <td>ION</td>\n",
              "      <td>THE TWO SATURN 2003 IONS I HAVE DRIVEN  (INCLUDING MY CURRENT VEHICLE) HAVE A TRANSMISSION PROBLEM WHERE, WHEN ENGAGING IN THIRD GEAR, THE TRANSMISSION WILL \"FREEWHEEL\" FOR SEVERAL SECONDS BEFORE ENGAGING WITH A LURCH.  THE PROBLEM CAN BE CREATED BY ACCELERATING AROUND A CORNER WHILE THE TRANSMISSION IS SHIFTING FROM SECOND TO THIRD.  THE CARS WERE  MANUFACTURED IN MARCH/APRIL, 2003.  *AK</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>10600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   nthsa_id  Year    make  ... crashed abs  mileage\n",
              "0  10022578  2003  SATURN  ...       N   N      NaN\n",
              "1  10040419  2003  SATURN  ...       N   N      NaN\n",
              "2  10042851  2003  SATURN  ...       N   N    500.0\n",
              "3  10049638  2003  SATURN  ...       N   Y  10600.0\n",
              "\n",
              "[4 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nthsa_id         int64\n",
              "Year             int64\n",
              "make            object\n",
              "model           object\n",
              "description     object\n",
              "crashed         object\n",
              "abs             object\n",
              "mileage        float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux7RwhsGRKKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Setup Program Constraints\n",
        "n_comments = len(df['description']) #Number of Complaints\n",
        "m_features = None                   #Number of SVD vectors\n",
        "s_words = 'english'                 #Stop Words Dictionary\n",
        "comments = df['description']        \n",
        "n_topics = 9                        #Number of topic clusters to extract\n",
        "max_iter = 10                       #Maximum number of iterations\n",
        "max_df = 0.5                        #Learning offset for LDAmax proportion \n",
        "                                      #of docs/reviews allowed for a term"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9KuCaawRKK9",
        "colab_type": "text"
      },
      "source": [
        "Tokenization, POS Tagging, Stopwords Removal and Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jwrtaj6RKLC",
        "colab_type": "code",
        "outputId": "b0bf2b5e-058d-4904-c1af-2026a2581b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "##Create word frequency by Review Matrix using Custom Analyzer\n",
        "cv = CountVectorizer(max_df=0.95, min_df =2, max_features=m_features,\n",
        "                    analyzer=my_analyzer, ngram_range=(1,2))\n",
        "tf = cv.fit_transform(comments)\n",
        "terms = cv.get_feature_names()\n",
        "term_sums = tf.sum(axis=0)\n",
        "term_counts = []\n",
        "for i in range(len(terms)):\n",
        "    term_counts.append([terms[i], term_sums[0,i]])\n",
        "def sortSecond(e):\n",
        "    return e[1]\n",
        "term_counts.sort(key=sortSecond, reverse=True)\n",
        "print(\"\\nTerms with Highest Frequency:\")\n",
        "for i in range(10):\n",
        "    print('{:<15s}{:>5d}'.format(term_counts[i][0], term_counts[i][1]))\n",
        "print(\"\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Terms with Highest Frequency:\n",
            "vehicle         6996\n",
            "steer           2924\n",
            "contact         2604\n",
            "power           2131\n",
            "failure         1745\n",
            "drive           1670\n",
            "problem         1466\n",
            "chevrolet       1422\n",
            "turn            1256\n",
            "recall          1239\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgAyM1TYRKLO",
        "colab_type": "text"
      },
      "source": [
        "Create TFIDF Matrix\n",
        "    TFIDF is created by transforming the term frequency matrix tf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItOXacD0RKLR",
        "colab_type": "code",
        "outputId": "4ccd2b4f-86e9-4cae-c7fc-b9eb7782c98d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "##Modify tf - term_frequencies to TF/IDF matrix from the data\n",
        "print(\"Conducting Term/Frequency Matrix usinf TF-IDF\")\n",
        "tfidf_vect = TfidfTransformer(norm=None, use_idf=True)\n",
        "tf = tfidf_vect.fit_transform(tf)\n",
        "\n",
        "term_idf_sums = tf.sum(axis=0)\n",
        "term_idf_scores = []\n",
        "for i in range(len(terms)):\n",
        "    term_idf_scores.append([terms[i], term_idf_sums[0,i]])\n",
        "print(\"The term/Frequency Matrix has\", tf.shape[0], \"rows, and \", tf.shape[1], \"columns.\")\n",
        "print(\"The Term list has\", len(terms), \" terms.\")\n",
        "term_idf_scores.sort(key=sortSecond, reverse=True)\n",
        "print(\"\\nTerms with highest TF-IDF Scores:\")\n",
        "for i in range(10):\n",
        "    print('{:<15s}{:>8.2f}'.format(term_idf_scores[i][0],term_idf_scores[i][1]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conducting Term/Frequency Matrix usinf TF-IDF\n",
            "The term/Frequency Matrix has 2734 rows, and  3276 columns.\n",
            "The Term list has 3276  terms.\n",
            "\n",
            "Terms with highest TF-IDF Scores:\n",
            "vehicle         8162.39\n",
            "contact         5371.28\n",
            "steer           5114.56\n",
            "power           4081.67\n",
            "failure         3553.85\n",
            "problem         3208.86\n",
            "drive           2958.99\n",
            "recall          2788.68\n",
            "turn            2765.90\n",
            "go              2757.42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX7YVYiQRKLa",
        "colab_type": "text"
      },
      "source": [
        "Singular Value Decomposition\n",
        "\n",
        "Use SVD to decompose the TFIDF matrix tf. This is called Latent Semantic Analysis - LSA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Unbp9HRKLd",
        "colab_type": "code",
        "outputId": "2b1ffd9e-f97e-4d37-d9fe-cb6117181e90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        }
      },
      "source": [
        "##SVD is synonymous with LSA in sklearn\n",
        "uv = TruncatedSVD(n_components=n_topics, algorithm='arpack',\n",
        "                 tol=0, random_state=9999)\n",
        "U = uv.fit_transform(tf)\n",
        "\n",
        "#Display the topic selections\n",
        "print(\"\\n*******Generated Topics*******\")\n",
        "display_topics(uv.components_, terms, n_terms=15)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "*******Generated Topics*******\n",
            "Topic #1: \n",
            "+vehicle       +steer         +contact       +power         +problem       \n",
            "+would         +go            +recall        +failure       +drive         \n",
            "+turn          +time          +start         +chevrolet     +gm            \n",
            "\n",
            "Topic #2: \n",
            "-contact       -failure       -mileage       -state         -own           \n",
            "+problem       -manufacturer  +go            -fuel          -repair        \n",
            "-current       +start         -campaign      +fix           -mph           \n",
            "\n",
            "Topic #3: \n",
            "-steer         -power         +fuel          +ignition      +key           \n",
            "+start         +pump          -drive         +switch        +leak          \n",
            "-go            +saturn        -turn          -wheel         +smell         \n",
            "\n",
            "Topic #4: \n",
            "-power         -steer         +brake         +front         +tire          \n",
            "+air           +side          -fuel          +bag           +door          \n",
            "+vehicle       +deploy        +driver        -recall        +hit           \n",
            "\n",
            "Topic #5: \n",
            "+fuel          -ignition      -key           +pump          -start         \n",
            "-switch        +leak          +recall        +smell         +gasoline      \n",
            "-contact       -saturn        +tank          -would         +gas           \n",
            "\n",
            "Topic #6: \n",
            "+door          +open          +handle        -brake         -start         \n",
            "-vehicle       +side          +insid         +issue         +key           \n",
            "+driver        -saturn        +plastic       +safety        +passenger     \n",
            "\n",
            "Topic #7: \n",
            "-gm            -bag           -air           +fuel          +start         \n",
            "-recall        +tire          +brake         +vehicle       -part          \n",
            "-deploy        -problem       -crash         +door          +turn          \n",
            "\n",
            "Topic #8: \n",
            "+key           -start         -saturn        +ignition      -door          \n",
            "+remove        -problem       +gear          -ion           +release       \n",
            "-cold          +position      +gm            +turn          +shift         \n",
            "\n",
            "Topic #9: \n",
            "+tire          -fuel          +firestone     +tread         -door          \n",
            "-vehicle       +rim           +new           +tell          -deploy        \n",
            "-bag           -passenger     +saturn        -seat          +part          \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOACefA2RKLx",
        "colab_type": "text"
      },
      "source": [
        "Add Topic Scores to Dataframe\n",
        "The matrix U contains the SVD calculations that can be used to assign each document to a topic group.\n",
        "\n",
        "The code below examines the matrix U and assigns topic groups to each document, then augments the original dataframe with document topic group and U."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgB9ZhuyRKL3",
        "colab_type": "code",
        "outputId": "0f76fd2d-4681-4c84-8941-15a972b1f296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "##Store topic group for each doc in topics[]\n",
        "topics = [0]*n_comments\n",
        "topic_counts = [0]*(n_topics+1)\n",
        "for i in range(n_comments):\n",
        "    max = abs(U[i][0])\n",
        "    topics[i] = 0\n",
        "    for j in range(n_topics):\n",
        "        x = abs(U[i][j])\n",
        "        if x > max:\n",
        "            max = x\n",
        "            topics[i] = j\n",
        "    topic_counts[topics[i]] += 1\n",
        "\n",
        "print('{:<6s}{:>8s}{:>8s}'.format(\"TOPIC\", \"COMMENTS\", \"PERCENT\"))\n",
        "for i in range(n_topics):\n",
        "    print('{:>3d}{:>10d}{:>8.1%}'.format((i+1), topic_counts[i], topic_counts[i]/n_comments))\n",
        "    \n",
        "##Create comment_scores[] and assign the topic groups\n",
        "comment_scores=[]\n",
        "for i in range(n_comments):\n",
        "    u = [0]*(n_topics+1)\n",
        "    u[0] = topics[i]\n",
        "    for j in range(n_topics):\n",
        "        u[j+1] = U[i][j]\n",
        "    comment_scores.append(u)\n",
        "\n",
        "##Augment Dataframe with topic group information\n",
        "cols = ['topic']\n",
        "for i in range(n_topics):\n",
        "    s = \"T\"+str(i+1)\n",
        "    cols.append(s)\n",
        "df_topics = pd.DataFrame.from_records(comment_scores, columns = cols)\n",
        "df = df.join(df_topics)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOPIC COMMENTS PERCENT\n",
            "  1      1777   65.0%\n",
            "  2       417   15.3%\n",
            "  3       103    3.8%\n",
            "  4       156    5.7%\n",
            "  5       119    4.4%\n",
            "  6        51    1.9%\n",
            "  7        37    1.4%\n",
            "  8        48    1.8%\n",
            "  9        26    1.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxEqBxGiRKMG",
        "colab_type": "text"
      },
      "source": [
        "    Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76giXkGsRKMK",
        "colab_type": "code",
        "outputId": "b844945a-c4fa-4461-9000-7d1bff4488ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "source": [
        "df.head(1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nthsa_id</th>\n",
              "      <th>Year</th>\n",
              "      <th>make</th>\n",
              "      <th>model</th>\n",
              "      <th>description</th>\n",
              "      <th>crashed</th>\n",
              "      <th>abs</th>\n",
              "      <th>mileage</th>\n",
              "      <th>topic</th>\n",
              "      <th>T1</th>\n",
              "      <th>T2</th>\n",
              "      <th>T3</th>\n",
              "      <th>T4</th>\n",
              "      <th>T5</th>\n",
              "      <th>T6</th>\n",
              "      <th>T7</th>\n",
              "      <th>T8</th>\n",
              "      <th>T9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10022578</td>\n",
              "      <td>2003</td>\n",
              "      <td>SATURN</td>\n",
              "      <td>ION</td>\n",
              "      <td>WHILE TRAVELING ON THE HIGHWAY AND WITHOUT PRIOR WARNING SEAT BELT RETRACTOR FELL APART.  *AK  THE BOLT THAT CONNECTS THE WEBBING TO THE FLOOR WAS NOT FULLY SCREWED IN AT THE PLANT.  THE BOLT BACKED OUT AND THE LOWER PORTION OF THE SEATBELT WEBBING BECAME UNATTACHED.  THIS IS NOT A BUCKLE ISSUE OR A RETRACTOR ISSUE.  MANUFACTURING DEFECT FROM THE PLANT BECAUSE THE BOLT WAS NOT FULLY TORQUED.  DEALER FIXED BY TIGHTENING THE BOLT.  CW</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2.762473</td>\n",
              "      <td>1.051344</td>\n",
              "      <td>0.099453</td>\n",
              "      <td>1.183402</td>\n",
              "      <td>0.748171</td>\n",
              "      <td>0.751461</td>\n",
              "      <td>-0.820782</td>\n",
              "      <td>0.096164</td>\n",
              "      <td>-1.021086</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   nthsa_id  Year    make model  ...        T6        T7        T8        T9\n",
              "0  10022578  2003  SATURN   ION  ...  0.751461 -0.820782  0.096164 -1.021086\n",
              "\n",
              "[1 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAMAeXtQRKKb",
        "colab_type": "text"
      },
      "source": [
        "Attribute Map for Preprocessing Data:\n",
        "\n",
        "The following attribute map describes the data features.\n",
        "The attribute 'crashed' is the target.\n",
        "The attribute 'description' contains the text(driver's complaint)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80ehpk_kRKKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attribute_map = {\n",
        "    'nthsa_id':['Z',(0,1e+12),[0,0]],\n",
        "    'Year':['N',(2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011),[0,0]],\n",
        "    'make':['N',('CHEVROLET', 'PONTIAC', 'SATURN'),[0,0]],\n",
        "    'model':['N',('COBALT', 'G5', 'HHR', 'ION', 'SKY', 'SOLSTICE'),[0,0]],\n",
        "    'description':['Z',(''),[0,0]],\n",
        "    'crashed':['B',('N', 'Y'),[0,0]],\n",
        "    'abs':['B',('N', 'Y'),[0,0]],\n",
        "    'mileage':['I',(1,200000),[0,0]],\n",
        "    'topic':['N',(0,1,2,3,4,5,6,7,8),[0,0]],\n",
        "    'T1':['I',(-1e+8,1e+8),[0,0]],\n",
        "    'T2':['I',(-1e+8,1e+8),[0,0]],\n",
        "    'T3':['I',(-1e+8,1e+8),[0,0]],\n",
        "    'T4':['I',(-1e+8,1e+8),[0,0]],\n",
        "    'T5':['I',(-1e+8,1e+8),[0,0]],\n",
        "    'T6':['I',(-1e+8,1e+8),[0,0]],\n",
        "    'T7':['I',(-1e+8,1e+8),[0,0]],\n",
        "    'T8':['I',(-1e+8,1e+8),[0,0]],\n",
        "    'T9':['I',(-1e+8,1e+8),[0,0]]\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8sWaQlQRKKs",
        "colab_type": "text"
      },
      "source": [
        "Attributes with '2' as the first number are nominal.\n",
        "The topic attribute is the text topic cluster number. The attributes T1-T9 are the scores for the individual documents for the topic cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyThXF7-RKMW",
        "colab_type": "code",
        "outputId": "0d12fd9e-ace7-410a-c44f-e241897bed9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "target = 'crashed'\n",
        "##Drop data with missing values for target\n",
        "drops = []\n",
        "for i in range(df.shape[0]):\n",
        "    if pd.isnull(df['crashed'][i]):\n",
        "        drops.append(i)\n",
        "df = df.drop(drops)\n",
        "df = df.reset_index()\n",
        "\n",
        "encoding = 'one-hot'\n",
        "scale = None ##Interval Scaling\n",
        "rie = ReplaceImputeEncode(data_map=attribute_map, nominal_encoding=encoding,\n",
        "                         interval_scale=scale, drop = True, display = True)\n",
        "encoded_df = rie.fit_transform(df)\n",
        "\n",
        "varlist = [target, 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9']\n",
        "X = encoded_df.drop(varlist, axis = 1)\n",
        "y = encoded_df[target]\n",
        "np_y = np.ravel(y) #Convert dataframe to flat array\n",
        "col = rie.col\n",
        "for i in range(len(varlist)):\n",
        "    col.remove(varlist[i])\n",
        "\n",
        "lr = LogisticRegression(C=1e+16, tol=1e-16)\n",
        "lr = lr.fit(X, np_y)\n",
        "\n",
        "logreg.display_coef(lr, X.shape[1], 2, col)\n",
        "logreg.display_binary_metrics(lr, X, y)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "********** Data Preprocessing ***********\n",
            "Features Dictionary Contains:\n",
            "10 Interval, \n",
            "2 Binary, \n",
            "4 Nominal, and \n",
            "3 Excluded Attribute(s).\n",
            "\n",
            "Data contains 2734 observations & 19 columns.\n",
            "\n",
            "\n",
            "Attribute Counts\n",
            ".................. Missing  Outliers\n",
            "nthsa_id.....         0         0\n",
            "Year.........         0         0\n",
            "make.........         0         0\n",
            "model........         0         0\n",
            "description..         0         0\n",
            "crashed......         0         0\n",
            "abs..........        18         0\n",
            "mileage......       419        15\n",
            "topic........         0         0\n",
            "T1...........         0         0\n",
            "T2...........         0         0\n",
            "T3...........         0         0\n",
            "T4...........         0         0\n",
            "T5...........         0         0\n",
            "T6...........         0         0\n",
            "T7...........         0         0\n",
            "T8...........         0         0\n",
            "T9...........         0         0\n",
            "\n",
            "Coefficients:\n",
            "Intercept..        -1.2817\n",
            "mileage....        -0.0000\n",
            "abs........         0.4754\n",
            "Year2003...        -0.4768\n",
            "Year2004...        -0.4521\n",
            "Year2005...        -0.7982\n",
            "Year2006...        -0.5902\n",
            "Year2007...        -0.4663\n",
            "Year2008...        -0.1779\n",
            "Year2009...        -0.1677\n",
            "Year2010...        -0.5512\n",
            "make0:CHEV.         0.0666\n",
            "make1:PONT.        -0.2118\n",
            "model0:COBA         0.3784\n",
            "model1:G5..         0.8296\n",
            "model2:HHR.        -0.3118\n",
            "model3:ION.        -0.4251\n",
            "model4:SKY.        -0.7114\n",
            "topic0.....         0.7365\n",
            "topic1.....         1.1021\n",
            "topic2.....        -1.5411\n",
            "topic3.....         3.6637\n",
            "topic4.....        -2.6275\n",
            "topic5.....        -0.4887\n",
            "topic6.....         1.6523\n",
            "topic7.....        -1.7170\n",
            "\n",
            "Model Metrics\n",
            "Observations...............      2734\n",
            "Coefficients...............        26\n",
            "DF Error...................      2708\n",
            "Mean Absolute Error........    0.3246\n",
            "Avg Squared Error..........    0.1621\n",
            "Accuracy...................    0.7721\n",
            "Precision..................    0.8000\n",
            "Recall (Sensitivity).......    0.2016\n",
            "F1-Score...................    0.3221\n",
            "MISC (Misclassification)...     22.8%\n",
            "     class 0...............      1.9%\n",
            "     class 1...............     79.8%\n",
            "\n",
            "\n",
            "     Confusion\n",
            "       Matrix     Class 0   Class 1  \n",
            "Class 0.....      1963        37\n",
            "Class 1.....       586       148\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw5uf64KRKMw",
        "colab_type": "text"
      },
      "source": [
        "    Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPROttVIRKM0",
        "colab_type": "code",
        "outputId": "ed813935-3136-440b-ee5a-760d9fbe01fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "scale = None\n",
        "rie = ReplaceImputeEncode(data_map=attribute_map, nominal_encoding=encoding,\n",
        "                         interval_scale=scale, drop=False, display=True)\n",
        "encoded_df = rie.fit_transform(df)\n",
        "varlist = [target, 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9']\n",
        "X = encoded_df.drop(varlist, axis = 1)\n",
        "y = encoded_df[target]\n",
        "np_y = np.ravel(y)\n",
        "col = rie.col\n",
        "for i in range(len(varlist)):\n",
        "    col.remove(varlist[i])\n",
        "\n",
        "dtc = DecisionTreeClassifier(max_depth=7, min_samples_split=5,\n",
        "                            min_samples_leaf=5)\n",
        "dtc = dtc.fit(X, np_y)\n",
        "DecisionTree.display_importance(dtc, col, plot=False)\n",
        "DecisionTree.display_binary_metrics(dtc, X, y)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "********** Data Preprocessing ***********\n",
            "Features Dictionary Contains:\n",
            "10 Interval, \n",
            "2 Binary, \n",
            "4 Nominal, and \n",
            "3 Excluded Attribute(s).\n",
            "\n",
            "Data contains 2734 observations & 19 columns.\n",
            "\n",
            "\n",
            "Attribute Counts\n",
            ".................. Missing  Outliers\n",
            "nthsa_id.....         0         0\n",
            "Year.........         0         0\n",
            "make.........         0         0\n",
            "model........         0         0\n",
            "description..         0         0\n",
            "crashed......         0         0\n",
            "abs..........        18         0\n",
            "mileage......       419        15\n",
            "topic........         0         0\n",
            "T1...........         0         0\n",
            "T2...........         0         0\n",
            "T3...........         0         0\n",
            "T4...........         0         0\n",
            "T5...........         0         0\n",
            "T6...........         0         0\n",
            "T7...........         0         0\n",
            "T8...........         0         0\n",
            "T9...........         0         0\n",
            "\n",
            "FEATURE.... IMPORTANCE\n",
            "topic3.....   0.4870\n",
            "mileage....   0.1513\n",
            "make2:SATU.   0.0896\n",
            "model2:HHR.   0.0408\n",
            "model5:SOLS   0.0367\n",
            "topic1.....   0.0364\n",
            "topic4.....   0.0362\n",
            "topic2.....   0.0333\n",
            "Year2005...   0.0184\n",
            "topic6.....   0.0163\n",
            "Year2007...   0.0159\n",
            "Year2006...   0.0098\n",
            "abs........   0.0089\n",
            "topic0.....   0.0081\n",
            "make0:CHEV.   0.0062\n",
            "model0:COBA   0.0041\n",
            "Year2004...   0.0012\n",
            "Year2003...   0.0000\n",
            "Year2008...   0.0000\n",
            "Year2009...   0.0000\n",
            "Year2010...   0.0000\n",
            "Year2011...   0.0000\n",
            "make1:PONT.   0.0000\n",
            "model1:G5..   0.0000\n",
            "model3:ION.   0.0000\n",
            "model4:SKY.   0.0000\n",
            "topic5.....   0.0000\n",
            "topic7.....   0.0000\n",
            "topic8.....   0.0000\n",
            "\n",
            "\n",
            "Model Metrics\n",
            "Observations...............      2734\n",
            "Features...................        29\n",
            "Maximum Tree Depth.........         7\n",
            "Minimum Leaf Size..........         5\n",
            "Minimum split Size.........         5\n",
            "Mean Absolute Error........    0.3099\n",
            "Avg Squared Error..........    0.1549\n",
            "Accuracy...................    0.7831\n",
            "Precision..................    0.8406\n",
            "Recall (Sensitivity).......    0.2371\n",
            "F1-Score...................    0.3698\n",
            "MISC (Misclassification)...     21.7%\n",
            "     class 0...............      1.6%\n",
            "     class 1...............     76.3%\n",
            "\n",
            "\n",
            "     Confusion\n",
            "       Matrix     Class 0   Class 1  \n",
            "Class 0.....      1967        33\n",
            "Class 1.....       560       174\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}